##################################################
# ----- Kafka Connect as a REST API Server ----- #
##################################################

# Kafka Connect's REST API is how users create, view, and modify individual connectors.

listeners=https://kafka-connect:8083


# ----- TLS ----- #

# Keystore Used for HTTPS
ssl.keystore.location=/home/training/rbac/security/tls/kafka-connect/kafka-connect.keystore.p12
ssl.keystore.password=confluent
ssl.keystore.type=PKCS12
ssl.key.password=confluent



####################
# ----- RBAC ----- #
####################

# Kafka Connect's REST API uses token impersonation to manage access at the level of individual connectors

# Declare a security class so the REST server can impersonate its clients
rest.servlet.initializor.classes=io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler

# Connect to the Metadata Service
confluent.metadata.bootstrap.server.urls=https://mds:8090
confluent.metadata.http.auth.credentials.provider=BASIC
confluent.metadata.basic.auth.user.info=connect-cluster:connect-cluster-secret

# Use the MDS public key to verify tokens returned by MDS
public.key.path=/home/training/rbac/security/token/public.pem



##################################################
# ----- Kafka Connect as a Kafka Client ----- #
##################################################

# Kafka Connect workers interact with internal Kafka Topics,
# so it connects to Kafka like any other client.

# Connect worker uses Kafka's token listener so that it can authenticate REST API clients
#   and authorize requests to view, create, or modify connectors.
bootstrap.servers = kafka:9095
security.protocol=SASL_SSL
sasl.mechanism=OAUTHBEARER
sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler
sasl.jaas.config = org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  username="connect-cluster" \
  password="connect-cluster-secret" \
  metadataServerUrls="https://mds:8090";

# Group ID identifies group of Connect workers as a cluster.
# This property sets the RBAC cluster id used by --connect-cluster-id
group.id = connect-cluster

# Internal topics
config.storage.topic = connect-configs
offset.storage.topic = connect-offsets
status.storage.topic = connect-statuses


# ----- TLS ----- #

# The Connect worker needs to trust Kafka broker's certificate
ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
ssl.truststore.password=confluent



##################################
# ----- Connector Defaults ----- #
##################################

# Each connector is a job that moves data into/out of Kafka.
# Each connector has its own configs that supplement/override the defaults.
# Connectors are created, viewed, or modified with the REST API.


# ----- General Defaults ----- #

# Enable each connector to override default configs
connector.client.config.override.policy=All

# Converters specify the format of data in Kafka and how to translate it into Connect data.

# Default key (de)serializer is String
key.converter = org.apache.kafka.connect.storage.StringConverter
key.converter.schema.registry.url = https://schema-registry:8081
# Default value (de)serializer is Avro
value.converter = io.confluent.connect.avro.AvroConverter
# Track schemas in Schema Registry
value.converter.schema.registry.url = https://schema-registry:8081
# Trust the Schema Registry certificate
value.converter.schema.registry.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
value.converter.schema.registry.ssl.truststore.password=confluent


# ----- Default Producer Config (Source Connectors) ----- #

# A source connector creates a producer on the worker which is a Kafka client like any other.
# For simplicity, we use the same bootstrap.servers as the worker, which means using 
#   Kafka's token listener even though there is no token impersonation happening.
# Omit producer.sasl.jaas.config so that each individual connector must supply its own credentials.
producer.security.protocol=SASL_SSL
producer.sasl.mechanism=OAUTHBEARER
producer.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler

# The client must trust the Kafka broker's certificate
producer.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
producer.ssl.truststore.password=confluent


# ----- Default Consumer Config (Sink Connectors) ----- #

# A sink connector creates a consumer on the worker which is a Kafka client like any other.
# For simplicity, we use the same bootstrap.servers as the worker, which means using 
#   Kafka's token listener even though there is no token impersonation happening.
# Again, omit consumer.sasl.jaas.config so that each individual connector must supply its own credentials.
consumer.security.protocol=SASL_SSL
consumer.sasl.mechanism=OAUTHBEARER
consumer.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler

# The client must trust the Kafka broker's certificate
consumer.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
consumer.ssl.truststore.password=confluent


# ----- Default Admin Config ----- #

# Sink connectors sometimes need to create a "dead letter queue" topic
#   to report errors, which requires an admin client.
# Again, omit admin.sasl.jaas.config so that each individual connector must supply its own credentials.
admin.security.protocol=SASL_SSL
admin.sasl.mechanism=OAUTHBEARER
admin.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler

# The client must trust the Kafka broker's certificate
admin.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
admin.ssl.truststore.password=confluent



#####################################################
# ----- Kafka Connect Monitoring Interceptors ----- #
#####################################################

# Kafka Connect sends metrics to Confluent Control Center by producing to an internal topic with monitoring interceptors.
# Here, we use the connect-cluster service principal, but you could configure a principal specifically for monitoring interceptors.


# ----- Monitoring Interceptor for Source Connectors ----- #

producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
# Sometimes, the monitoring cluster is a different cluster enitrely, so you can specify different bootstrap.servers.
# For simplicity, we use the same bootstrap.servers as the worker, which means using 
#   Kafka's token listener even though there is no token impersonation happening.
producer.confluent.monitoring.interceptor.bootstrap.servers=kafka:9095
producer.confluent.monitoring.interceptor.security.protocol=SASL_SSL
producer.confluent.monitoring.interceptor.sasl.mechanism=OAUTHBEARER
producer.confluent.monitoring.interceptor.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler
producer.confluent.monitoring.interceptor.sasl.jaas.config = org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  username="connect-cluster" \
  password="connect-cluster-secret" \
  metadataServerUrls="https://mds:8090";

# The monitoring interceptor must trust the Kafka broker's certificate.
producer.confluent.monitoring.interceptor.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
producer.confluent.monitoring.interceptor.ssl.truststore.password=confluent


# ----- Monitoring Interceptor for Sink Connectors ----- #

consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
# Sometimes, the monitoring cluster is a different cluster enitrely, so you can specify different bootstrap.servers.
# For simplicity, we use the same bootstrap.servers as the worker, which means using 
#   Kafka's token listener even though there is no token impersonation happening.
consumer.confluent.monitoring.interceptor.bootstrap.servers=kafka:9095
consumer.confluent.monitoring.interceptor.security.protocol=SASL_SSL
consumer.confluent.monitoring.interceptor.sasl.mechanism=OAUTHBEARER
consumer.confluent.monitoring.interceptor.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler
consumer.confluent.monitoring.interceptor.sasl.jaas.config = org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  username="connect-cluster" \
  password="connect-cluster-secret" \
  metadataServerUrls="https://mds:8090";

# The monitoring interceptor must trust the Kafka broker's certificate.
consumer.confluent.monitoring.interceptor.ssl.truststore.location=/home/training/rbac/security/tls/kafka-client/kafka-client.truststore.jks
consumer.confluent.monitoring.interceptor.ssl.truststore.password=confluent



#############################################
# ----- Kafka Connect Secret Registry ----- #
#############################################

# Connect Secret Registry is used to manage connector secrets.

# "secret" is for Connect Secret Registry (storing and retrieving connector credentials)
# "securepass" is for Confluent Secrets (encrypting properties files)
config.providers = securepass,secret
config.providers.securepass.class = io.confluent.kafka.security.config.provider.SecurePassConfigProvider
config.providers.secret.class=io.confluent.connect.secretregistry.rbac.config.provider.InternalSecretConfigProvider

# Declare classes that allow REST server to use Secret Registry API extentions
rest.extension.classes=io.confluent.connect.security.ConnectSecurityExtension,io.confluent.connect.secretregistry.ConnectSecretRegistryExtension

# Define an encryption password
config.providers.secret.param.master.encryption.key=connect-cluster-secret

# Connect to Kafka's token listener for token impersonation
config.providers.secret.param.kafkastore.bootstrap.servers=SASL_SSL://kafka:9095
config.providers.secret.param.kafkastore.security.protocol=SASL_SSL
config.providers.secret.param.kafkastore.sasl.mechanism=OAUTHBEARER
config.providers.secret.param.kafkastore.sasl.login.callback.handler.class=io.confluent.kafka.clients.plugins.auth.token.TokenUserLoginCallbackHandler
config.providers.secret.param.kafkastore.sasl.jaas.config=org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
  username="connect-cluster" \
  password="connect-cluster-secret" \
  metadataServerUrls="https://mds:8090";



#############################
# ----- Miscellaneous ----- #
#############################

# These configs are not the focus of these training materials.

config.storage.replication.factor = 1
offset.storage.replication.factor = 1
status.storage.replication.factor = 1
internal.key.converter = org.apache.kafka.connect.json.JsonConverter
internal.value.converter = org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable = false
internal.value.converter.schemas.enable = false

# Set to a list of filesystem paths separated by commas (,) to enable class loading isolation for plugins
plugin.path = /usr/share/java,/usr/share/confluent-hub-components